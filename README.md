# ğŸ§  Mini LLM â€” Character-Level Multilingual Language Model

This is a lightweight character-level LLM (Large Language Model) built from scratch using Python and PyTorch.  
Originally designed for simple story generation, it now supports **Shona**, **English**, and **Venda** translation-style prompts using custom datasets.

---

## âœ¨ Features

- ğŸ§± Built from scratch with PyTorch and pure Python.
- ğŸ§  Learns from **raw text** (fantasy stories, translations, etc.)
- ğŸ” Trains on your own `train.txt` and `val.txt`.
- ğŸŒ Supports **translation-style completion** across 3 languages:
  - Shona  
  - English  
  - Venda
- ğŸ’¾ Saves model weights, vocab, and metadata.
- ğŸ“ˆ Generates coherent sequences **character-by-character**.

---

## ğŸ“ Project Structure

Mini LLM/
â”œâ”€â”€ train.py # Trains the model and saves weights
â”œâ”€â”€ generate.py # Generates text from the trained model
â”œâ”€â”€ model.py # Transformer-based mini LLM definition
â”œâ”€â”€ train.txt # Mixed training data
â”œâ”€â”€ val.txt # Validation data
â”œâ”€â”€ trained_llm.pth # Saved model weights
â”œâ”€â”€ data/ # Folder for cleaned Shona & Venda corpora
â””â”€â”€ README.md # This file

## ğŸ§ª How It Works

### 1. Training (`train.py`)
- Reads `train.txt` and `val.txt`
- Tokenizes characters â†’ integer IDs
- Trains a **mini transformer model** on the data
- Saves model weights as `trained_llm.pth`

### 2. Generation (`generate.py`)
- Loads the model + vocab
- Accepts a prompt (e.g. `"Shona: Ndinokuda.\nEnglish:"`)
- Generates text for continuation (e.g., translation)
- Uses temperature sampling and softmax decoding

---

## ğŸŒ Example Translation-Style Prompts

Shona: Ndinokuda.
English: I love you.
Venda: Ndi a ni funa.

Output will vary depending on training data and tokens.

---

## ğŸ›  Requirements

- Python 3.8+
- PyTorch

Install PyTorch:
pip install torch


---

## ğŸ“Œ Notes

- This model is small and lightweight. Results improve as you train longer and feed more data.
- Supports multi-language prompts using custom datasets.
- Future improvements: Attention scoring, better tokenization, grammar correction.

---

## ğŸ”® Future Ideas

- Word-level tokenizer (instead of character-based).
- Expand to Swahili, Zulu, or other African languages.
- Real-time translation UI or agent with microphone input.

